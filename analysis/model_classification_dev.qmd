---
title: "Model classification in python dev"
format: gfm
jupyter: python3
execute: 
  warning: false
  error: false
  eval: false
---

## Setup
```{python}
#| label: setup
from glob import glob
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, roc_auc_score, ConfusionMatrixDisplay, RocCurveDisplay
from skopt import BayesSearchCV
from skopt import dump as skdump, load as skload
from skopt.space import Real, Integer
from skopt.plots import plot_convergence, plot_evaluations
# for CPU machines:
## mamba install -c conda-forge py-xgboost-cpu
# for GPU machines (when I run remote):
## mamba install -c conda-forge py-xgboost-gpu
from xgboost import XGBClassifier
```

## Set random seed

```{python}
#| label: rand-seed

## ihe: 1112
## cat: 21212121
## per: 3333
np.random.seed(21212121)

```

## Import and clean data

```{python}
# function to import data for model classification
def import_data(folder: str, transformation: str) -> pd.DataFrame:

  # specify path to sumstats
  sumstats_paths = glob(folder + "/*sumstats.csv")

  # only concatenate if there is more than one file
  if len(sumstats_paths) > 1:
    sumstats = pd.concat((pd.read_csv(f) for f in sumstats_paths), ignore_index=True)
  else:
    sumstats = pd.read_csv(sumstats_paths)
  

  # remove the columns we're not using for analysis (per-deme pi, the third hill number, morans I, IDs)
  sumstats = sumstats.loc[:,~sumstats.columns.str.startswith('pi_pop')]
  sumstats = sumstats.loc[:,~sumstats.columns.str.startswith('sfs_h3')]
  sumstats = sumstats.drop(columns = "morans_i")
  sumstats = sumstats.loc[:,~sumstats.columns.str.contains("_id")]

  sumstats["transformation"] = transformation


  return sumstats

# path order must correspond with transformation order
def combine_sumstats(folder_list: list, transformation_list = list) -> pd.DataFrame:
  
  ss_list = []
  for p in range(len(folder_list)):
    ss = import_data(folder_list[p], transformation=transformation_list[p])
    ss_list.append(ss)

  sumstats = pd.concat(ss_list, ignore_index = True)

  return sumstats


flist = ["output/simulations/sims_cat_linear", "output/simulations/sims_cat_threshold"]

tlist = ["linear", "threshold"]

sumstats = combine_sumstats(folder_list = flist, transformation_list=tlist)

```

## Split into train/test split

```{python}
#| label: train-test-split

# set predictor and response matrices
X = sumstats.drop(columns = "transformation")
y1 = sumstats["transformation"]

# Encode the labels into unique integers
encoder = LabelEncoder()
y = encoder.fit_transform(np.ravel(y1))

# get a seed based on the global seed for input into function
split_seed = np.random.randint(1, 1e6)

X_train, X_test, y_train, y_test = train_test_split(
  X, y, 
  test_size=0.2, 
  random_state=split_seed)


```


## Baseline model

Creating a baseline model to compare the tuned model to. Used somewhat arbitrary values to initialize it with.

> E. iheringii: the accuracy on training data (no CV) is 0.818 and the AUC is 0.90. The test accuracy is 0.760 and AUC is 0.84. It's overfit to the training data. 
> E. catenatus: training acc: 0.732. training AUC: 0.812. test acc: 0.651. test AUC: 0.713.

```{python}
#| label: base-model
pipe_baseline = make_pipeline(
  StandardScaler(),
  XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.5, objective='binary:logistic')
)

pipe_baseline.fit(X_train, y_train)

# predict to training
y_pred = pipe_baseline.predict(X_train)
y_pred_prob = pipe_baseline.predict_proba(X_train)

train_acc = accuracy_score(y_train, y_pred)
train_auc = roc_auc_score(y_train, y_pred_prob[:,1])

# predict to testing
y_pred_test = pipe_baseline.predict(X_test)
y_pred_prob_test = pipe_baseline.predict_proba(X_test)

test_acc = accuracy_score(y_test, y_pred_test)
test_auc = roc_auc_score(y_test, y_pred_prob_test[:,1])

```

## Create modeling pipeline
Using `make_pipeline` so I ensure that centering and scaling is applied consistently across CV folds, training, and testing data.


### GPU pipeline
```{python}
#| label: create-pipeline-gpu
pipe = make_pipeline(
  StandardScaler(),
  XGBClassifier(tree_method = "hist", device = "cuda", updater = "grow_gpu_hist", eval_metric = "auc")
)

pipe
```

### CPU pipeline

```{python}
#| label: create-pipeline-cpu

## Create modeling pipeline
  pipe = make_pipeline(
    StandardScaler(),
    XGBClassifier(tree_method = "hist", eval_metric = "auc")
  )

```

## Tune
I'm using Bayesian optimization to tune model hyperparameters.

```{python}
#| label: set-parameters

hparams = {
  "xgbclassifier__n_estimators": Integer(100, 2000),
  "xgbclassifier__eta": Real(1e-3, 1, "log-uniform"),
  "xgbclassifier__gamma": Real(1e-10, 1.5, "log-uniform"),
  "xgbclassifier__max_depth": Integer(1, 15),
  "xgbclassifier__subsample": Real(0.1, 1, "log-uniform"),
  "xgbclassifier__colsample_bytree": Real(0.01, 0.2, "log-uniform"),
  "xgbclassifier__alpha": Real(0, 0.5, "log-uniform")
}

```


```{python}
#| label: tune

# get a seed based on the global seed for input into function
cv_seed = np.random.randint(1, 1e6)


searchcv = BayesSearchCV(
  pipe,
    search_spaces=hparams,
    n_iter = 50,
    cv = 5,
    scoring = "roc_auc",
    verbose=5,
    random_state=cv_seed,
    n_jobs = 5
)

np.int = int # have to do this because skopt hasn't totally updated their use of np.int vs int
searchcv.fit(X_train, y_train)
```

## Write tuning results to file

```{python}
#| label: dump-tune
cv_out_path = "cv_results.pkl"

skdump(searchcv, out_path)

```

## Evaluate tuning


```{python}
#| label: import-tune

searchcv = skload(cv_out_path)
```


### Check for convergence
```{python}
#| label: plot-convergence
plot_convergence(searchcv.optimizer_results_)

```

### Select the best model
```{python}
best_pipeline = searchcv.best_estimator_

```

### Fit the best model to the full training data
```{python}
#| label: fit-training
best_pipeline.fit(X_train, y_train)
```

### Predict to the test data
Using binary predictions and probability predictions.
```{python}
#| label: pred-test
test_pred_bin = best_pipeline.predict(X_test)
test_pred_prob = best_pipeline.predict_proba(X_test)
```


AUC of the test data.

```{python}
#| label: roc-auc-test
roc_auc_score(y_test, test_pred_prob[:,1])
```

ROC curve plot.

```{python}
#| label: roc-curve-plot
RocCurveDisplay.from_predictions(y_test, test_pred_prob[:,1])
```


Accuracy of the test data.
```{python}
#: label: accuracy-test
accuracy_score(y_test, test_pred_bin)
```

Confusion matrix of the test data.

"0" is a linear model, "1" is a threshold model
```{python}
#| label: conf-mat-test

ConfusionMatrixDisplay.from_predictions(y_test, test_pred_bin, labels = best_pipeline.classes_, normalize = "true")
```


## Predict empirical data

```{python}
#| label: read-filter-func

def read_empirical(path):

  empirical_df = pd.read_csv(path)

  # only keep relevant summary statistics
  empirical_df = empirical_df.loc[:, empirical_df.columns.str.startswith(('pi_a', 'pi_sd_a', 'taj', 'sfs', 'dxy', 'ibd'))]

  # remove columns that contain "fst" or "h3"
  empirical_df = empirical_df.loc[:, ~empirical_df.columns.str.contains('fst|h3')]

  # remove dxy from column names
  empirical_df = empirical_df.rename(columns=lambda x: x.replace('_dxy', '') if x.startswith('ibd') else x)
  
  return empirical_df



```

### E. iheringii

```{python}
#| label: read-filter-emp-ihe
emp_ihe_path = "output/empirical_sumstats/iheringii_sumstats.csv"

emp_ihe = read_empirical(emp_ihe_path)

```

```{python}
#| label: pred-prob-emp-ihe
best_pipeline.predict_proba(emp_ihe)
```

## E. catenatus

```{python}
#| label: read-filter-emp-cat
emp_cat_path = "output/empirical_sumstats/catenatus_sumstats.csv"


emp_cat = read_empirical(emp_cat_path)

```


```{python}
#| label: pred-prob-emp-cat
best_pipeline.predict_proba(emp_cat)

```

